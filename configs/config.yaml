project:
  name: "ParaDetect"
  version: "1.0.0"
  environment: "development"  # development, staging, production

data:
  raw_data_dir: "data/raw/"
  processed_data_dir: "data/processed/"
  interim_data_dir: "data/interim/"

# Data Ingestion Configuration
data_ingestion:
  dataset_name: "artem9k/ai-text-detection-pile"
  source_type: "huggingface"  # huggingface, local, url
  raw_data_dir: "data/raw/"
  dataset_filename: "ai_text_detection_pile.csv"
  sample_size: null  # For development/testing, use a smaller sample
  random_state: 42

# Data Preprocessing Configuration
data_preprocessing:
  text_column: "text"
  label_column: "generated"
  source_column: "source"
  remove_duplicates: true
  min_text_length: 50
  max_text_length: 5000
  lowercase: false
  strip_whitespace: true
  remove_special_chars: false
  balance_classes: true
  processed_data_dir: "data/processed/"
  processed_filename: "ai_text_detection_pile_processed.csv"
  random_state: 42

# Data Validation Configuration
data_validation:
  expected_columns: ["text", "generated", "source"]
  required_columns: ["text", "generated"]
  text_column: "text"
  label_column: "generated"
  min_text_length: 10
  max_text_length: 10000
  expected_labels: [0, 1]
  min_samples_per_class: 100
  max_null_percentage: 0.05
  validation_report_dir: "artifacts/reports/"
  report_filename: "data_validation_report.json"

# Pipeline Configuration - State Management
pipeline:
  artifacts_dir: "artifacts/"
  checkpoints_dir: "artifacts/checkpoints/"
  state_files:
    data_pipeline: "artifacts/states/data_pipeline_state.json"
    training_pipeline: "artifacts/states/training_pipeline_state.json"
    inference_pipeline: "artifacts/states/inference_pipeline_state.json"
    monitoring_pipeline: "artifacts/states/monitoring_pipeline_state.json"
    deployment_pipeline: "artifacts/states/deployment_pipeline_state.json"
  enable_state_persistence: true
  state_auto_save: true
  state_retention_days: 30
  enable_pipeline_locks: true

# Inference Configuration
inference:
  # Model configuration
  model_path: null  # Path to trained model, null for latest
  tokenizer_path: null  # Path to tokenizer, null to use model_path
  device_preference: "auto"  # auto, cuda, mps, cpu

  # Inference parameters
  batch_size: 32
  max_length: 512

  # Processing configuration
  text_column: "text"
  preprocessing_enabled: true

  # Output configuration
  include_probabilities: true
  include_confidence: true
  confidence_threshold: 0.5

  # Monitoring configuration
  enable_monitoring: true
  log_predictions: false  # For privacy, default to False

  # Batch processing configuration
  chunk_size: 1000  # For large batch processing
  progress_bar: true

  # Error handling
  skip_errors: true
  max_retries: 3

  # API configuration
  api:
    host: "0.0.0.0"
    port: 8000
    workers: 1
    reload: false

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s [%(levelname)s] %(name)s (%(filename)s:%(lineno)d) - %(message)s"
  log_dir: "artifacts/logs/"

  # Component-specific logging
  loggers:
    data_pipeline:
      level: "INFO"
      handlers: ["console", "main_file"]
      propagate: false
    training_pipeline:
      level: "INFO"
      handlers: ["console", "main_file"]
      propagate: false
    inference_pipeline:
      level: "INFO"
      handlers: ["console", "main_file"]
      propagate: false
    model_training:
      level: "DEBUG"
      handlers: ["console", "main_file", "debug_file"]
      propagate: false

  # Enhanced log rotation with date-based naming
  rotation:
    when: "midnight"           # midnight, H, D, W0-W6
    interval: 1                # Rotate every interval
    backup_count: 7            # Keep 7 days of logs
    compress: true             # Compress old logs
    retention_days: 30         # Clean up logs older than 30 days

  # Structured logging for production
  structured: false            # Enable for production
  json_format: false          # Enable for production

# Monitoring Configuration
monitoring:
  enable: true
  metrics_dir: "artifacts/metrics/"
  track_data_drift: true
  track_model_performance: true

# Environment-specific overrides
environments:
  development:
    data_ingestion:
      sample_size: 1000  # Smaller sample for development
    inference:
      device_preference: "mps"
    logging:
      level: "DEBUG"
      rotation:
        backup_count: 3
        retention_days: 7
    # DeBERTa config overrides
    model_overrides:
      model_name_or_path: "microsoft/deberta-v3-base"
    lora_overrides:
      r: 8
      lora_alpha: 16
      bias: "none"
    tokenizer_overrides:
      max_length: 128
    # Training config overrides
    training_overrides:
      num_train_epochs: 1
      per_device_train_batch_size: 1
      per_device_eval_batch_size: 1
      gradient_accumulation_steps: 8
      learning_rate: 2e-5
      bf16: false
      dataloader_num_workers: 0
      gradient_checkpointing: true
      checkpoint_interval_steps: 50
      logging_steps: 2
      report_to: ["tensorboard"]  # ["wandb", "tensorboard"] or null
      run_name: "deberta-v3-base-finetuning"
      device_preference: "mps"
    # Dataset overrides
    dataset_overrides:
      max_length: 128
    # Evaluation overrides
    evaluation_overrides:
      eval_steps: 50
      save_steps: 50
    # Validation overrides
    validation_overrides:
      min_accuracy: 0.75
      min_f1: 0.75
      min_precision: 0.60
      min_recall: 0.60
      min_auc: 0.75
      calibration_bins: 10
      max_prediction_skew: 0.6
    registration_overrides:
      use_hf: false
      hf_repo_id: null
      hf_token: null
      push_to_hub: false
      use_mlflow: false
      use_sagemaker: false
      require_validation_pass: false
      registry_type: "local"
      local_registry_dir: "artifacts/model_registry/"
    inference_overrides:
      log_predictions: true
      enable_monitoring: false
      device_preference: "mps"
      batch_size: 8
      max_length: 128

  staging:
    data_ingestion:
      sample_size: 50000
    logging:
      level: "INFO"
      structured: true
      rotation:
        backup_count: 14
        retention_days: 30

  production:
    data_ingestion:
      sample_size: null  # Full dataset
    inference:
      device_preference: "cuda"
    logging:
      level: "WARNING"
      structured: true
      json_format: true
      rotation:
        backup_count: 30
        retention_days: 90
        compress: true
    inference:
      log_predictions: false
      enable_monitoring: true
    # DeBERTa config overrides
    model_overrides:
      model_name_or_path: "microsoft/deberta-v3-large"
      lora:
        r: 64
        lora_alpha: 128
        bias: "all"
      tokenizer:
        max_length: 512
    # Training config overrides
    training_overrides:
      num_train_epochs: 3
      per_device_train_batch_size: 32
      per_device_eval_batch_size: 32
      gradient_accumulation_steps: 1
      learning_rate: 2e-4
      fp16: true
      bf16: true
      dataloader_num_workers: 2
      gradient_checkpointing: true
      checkpoint_interval_steps: 250
      logging_steps: 100
      report_to: ["tensorboard"]  # ["wandb", "tensorboard"] or null
      run_name: "deberta-v3-large-finetuning"
      device_preference: "cuda"
    # Dataset overrides
    dataset_overrides:
      max_length: 512
    # Evaluation overrides
    evaluation_overrides:
      eval_steps: 500
      save_steps: 500
    # Validation overrides
    validation_overrides:
      min_accuracy: 0.85
      min_f1: 0.85
      min_precision: 0.80
      min_recall: 0.80
      min_auc: 0.85
      calibration_bins: 10
      max_prediction_skew: 0.8
    registration_overrides:
      use_hf: true
      use_mlflow: true
      use_sagemaker: true
      registry_type: "hf_hub"
